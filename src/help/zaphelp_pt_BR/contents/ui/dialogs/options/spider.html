<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Tela de opções do spider</title>
</head>
<body bgcolor="#ffffff">
	<h1>Tela de opções do spider</h1>
	<p>
		Esta tela permite que você configure as opções do <a<br>
			href="../../../start/concepts/spider.html">spider</a>.</p> 
	<p>Note-se que modificar a maioria dessas opções também afeta a execução do spider.
	</p>

	<h3>Profundidade máxima de busca</h3>
	O parâmetro define a profundidade máxima no processo de busca, no qual uma página deva ser encontrada para ser processada. Recursos encontrados mais abaixo deste nível não são buscados nem analisados pelo spider.
	<p>
		A profundidade é calculada a partir de 'sementes' (pontos de partida), então, se uma varredura do spider começa com apenas uma única URL (por exemplo, qualquer URL para o spider), a profundidade é calculada a partir deste nível. No entanto, se o exame começa com várias sementes (pontos de partida) um recurso é processado se a sua profundidade em relação a <i>qualquer</i> das sementes é menor do que aquele definido.
	</p>

	<h3>Número de segmentos (threads) usados</h3>
	O spider é multi-threaded e este é o número que define o número máximo de threads de trabalho utilizados no processo de rastreamento. Alterar este parâmetro não tem efeito sobre qualquer rastreamento já em andamento. 

	<h3>Maximum duration</h3>
	The maximum length of time that the spider should run for, measured in minutes.
	Zero (the default) means that the spider will run until it has found all of the links that it is able to. 

	<h3>Domain Pattern</h3>
	The normal behavior of the spider is to only follow links to resources
	found on the same domain as the page where the scan started. However,
	this option allows you to define additional domains that are considered
	"in scope" during the crawling process. Pages on these domains are
	processed during the scan.
		
	<h3>Query parameters handling</h3>
	When crawling, the Spider has an internal mechanism that marks which pages
	were already visited, so they are not processed again. When this check is made,
	the way the URIs parameters are handled is set using this option. There are
	three available options:
	<ul>
	<li><b>Ignorar parâmetros completamente</b> - se www.example.org/? barra = 456 é visitado, em seguida www.example.org/? foo = 123 não será visitado</li>
	<li><b>Considerar apenas o nome do parâmetro</b> (ignorar o valor do parâmetro) - se www.example.org/? foo = 123 é visitado, em seguida www.example.org/? foo = 456 não ser visitado, mas www.example.org/? barra = 789 ou www.example.org/? foo = 456 &amp; barra = 123 serão visitados</li>
	<li><b>Considerar tanto o nome quanto o valor do parâmetro</b> - se www.example.org/? 123 é visitada, algum outra URI diferente (incluindo, por exemplo, www.example.org/? foo = 456 ou www.example.org/? barra = abc) serão visitados</li>
	</ul>

	<h3>Send "Referer" header</h3>
	If the spider requests should be sent with the "Referer" header.

	<h3>Process forms</h3>
	During the crawling process, the behaviour of the spider when it
	encounters HTML forms is defined by this option. If disabled, the HTML
	forms will not be processed at all. If enabled, the HTML forms with the
	method defined as HTTP GET will be submitted with some generated
	values. The behaviour when encountering forms with the method defined
	as HTTP POST is configured by the next option.

	<h3>POST forms</h3>
	As briefly described in the previous paragraph (Process Forms), this
	option configures the behaviour of the spider when
	<i>Processar formulários</i> is enabled and when encountering HTML forms that
	have to be POSTed.

	<h3>Parse HTML Comments</h3>
	This option defines whether the spider should also spider the HTML
	comments searching for links to resources. Only the resources found in
	commented valid HTML tags will be processed.

	<h3>Parse 'robots.txt' files</h3>
	This option defines whether the spider should also spider the
	robots.txt files found on websites, searching for links to resources.
	This option does not define whether the spider should follow the rules
	imposed by the robots.txt file.

	<h3>Handle OData-specific parameters</h3>
	This options defines whether the spider should try to detect OData-specific
	parameters (i.e. resources identifiers) in order to properly process them 
	according to the rule defined by the "Query parameters handling" option.

	<h2>Leia também</h2>
	<table>
		<tr>
			<td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td><a href="../../overview.html">Visão geral da interface do usuário</a></td>
			<td>para uma visão geral da interface do usuário</td>
		</tr>
		<tr>
			<td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td><a href="../../../start/concepts/spider.html">Spider</a></td>
			<td>para uma visão geral do spider</td>
		</tr>
		<tr>
			<td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td><a href="../../../ui/tabs/spider.html">Aba Spider</a></td>
			<td>para uma visão geral do guia do spider</td>
		</tr>
	</table>

</body>
</html>
