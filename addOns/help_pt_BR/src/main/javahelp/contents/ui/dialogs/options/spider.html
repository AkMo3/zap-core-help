<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Tela de opções do spider</title>
</head>
<body bgcolor="#ffffff">
	<h1>Tela de opções do spider</h1>
	<p>
		Esta tela permite que você configure as opções do <a<br>
			href="../../../start/concepts/spider.html">spider</a>.</p> 
	<p>Note-se que modificar a maioria dessas opções também afeta a execução do spider.
	</p>

	<h3>Profundidade máxima de busca</h3>
	O parâmetro define a profundidade máxima no processo de busca, no qual uma página deva ser encontrada para ser processada. Recursos encontrados mais abaixo deste nível não são buscados nem analisados pelo spider.
	<p>
		A profundidade é calculada a partir de 'sementes' (pontos de partida), então, se uma varredura do spider começa com apenas uma única URL (por exemplo, qualquer URL para o spider), a profundidade é calculada a partir deste nível. No entanto, se o exame começa com várias sementes (pontos de partida) um recurso é processado se a sua profundidade em relação a <i>qualquer</i> das sementes é menor do que aquele definido.
	</p>

	<h3>Número de segmentos (threads) usados</h3>
	O spider é multi-threaded e este é o número que define o número máximo de threads de trabalho utilizados no processo de rastreamento. Alterar este parâmetro não tem efeito sobre qualquer rastreamento já em andamento. 

	<h3>Duração máxima</h3>
	A extensão máxima de tempo que o spider deve ser executado, medido em minutos.  Zero (o padrão) significa que o spider irá rodar até encontrar todos os links que conseguir. 

	<h3>Número máximo de nós "filhos" a percorrer</h3>
	Este parâmetro limita o número de "filhos" que serão rastreados em cada nó na árvore.<br>
	Isso é útil para aplicativos baseados em dados, que têm um grande número de 'páginas' contendo exatamente o mesmo código, mas com dados diferentes - por exemplo, de um banco de dados.<br>
	Por padrão, o número é definido como zero, o que significa que não há limites aplicados ao número de "filhos" rastreados.

	<h3>Maximum parse size</h3>
	Defines the maximum size, in bytes, that a response might have to be parsed. This allows
	the spider to skip big responses/files.

	<h3>Domains Always in Scope</h3>
	Allows to manage the domains, string literals or regular expressions, that are in the
	spider's scope. The normal behavior of the spider is to only follow links to resources
	found on the same domain as the page where the scan started. However,
	this option allows you to define additional domains that are considered
	"in scope" during the crawling process. Pages on these domains are
	processed during the scan.
		
	<h3>Query parameters handling</h3>
	When crawling, the Spider has an internal mechanism that marks which pages
	were already visited, so they are not processed again. When this check is made,
	the way the URIs parameters are handled is set using this option. There are
	three available options:
	<ul>
	<li><b>Ignorar parâmetros completamente</b> - se www.example.org/? barra = 456 é visitado, em seguida www.example.org/? foo = 123 não será visitado</li>
	<li><b>Considerar apenas o nome do parâmetro</b> (ignorar o valor do parâmetro) - se www.example.org/? foo = 123 é visitado, em seguida www.example.org/? foo = 456 não ser visitado, mas www.example.org/? barra = 789 ou www.example.org/? foo = 456 &amp; barra = 123 serão visitados</li>
	<li><b>Considerar tanto o nome quanto o valor do parâmetro</b> - se www.example.org/? 123 é visitada, algum outra URI diferente (incluindo, por exemplo, www.example.org/? foo = 456 ou www.example.org/? barra = abc) serão visitados</li>
	</ul>

	<h3>Send "Referer" header</h3>
	If the spider requests should be sent with the "Referer" header.

	<h3>Accept Cookies</h3>
	If the spider scans should accept cookies while spidering. If enabled the Spider will
	properly handle any cookies received from the server and will send them back accordingly.
	If the option is disabled, the Spider will not send any cookies in its requests. For
	example, this might control whether or not the Spider uses the same session throughout a
	spidering scan.
	<br>When accepting cookies the cookies are not shared between spider scans, each scan has
	its own cookie jar.
	<br>This option has low priority, the Spider will respect other (global) options related
	to the HTTP state. This option is ignored if, for example, the option
	<a href="../../tlmenu/edit.html">Enable Session Tracking (Cookie)</a> is enabled, when
	spidering as a <a href="../../../start/concepts/users.html">User</a> or when a
	<a href="../../../start/concepts/httpsessions.html">HTTP Session</a> is active.

	<h3>Process forms</h3>
	During the crawling process, the behaviour of the spider when it
	encounters HTML forms is defined by this option. If disabled, the HTML
	forms will not be processed at all. If enabled, the HTML forms with the
	method defined as HTTP GET will be submitted with some generated
	values. The behaviour when encountering forms with the method defined
	as HTTP POST is configured by the next option.

	<h3>POST forms</h3>
	As briefly described in the previous paragraph (Process Forms), this
	option configures the behaviour of the spider when
	<i>Processar formulários</i> is enabled and when encountering HTML forms that
	have to be POSTed.

	<h3>Parse HTML Comments</h3>
	This option defines whether the spider should also spider the HTML
	comments searching for links to resources. Only the resources found in
	commented valid HTML tags will be processed.

	<h3>Parse 'robots.txt' files</h3>
	This option defines whether the spider should also spider the
	robots.txt files found on websites, searching for links to resources.
	This option does not define whether the spider should follow the rules
	imposed by the robots.txt file.

	<h3>Handle OData-specific parameters</h3>
	This options defines whether the spider should try to detect OData-specific
	parameters (i.e. resources identifiers) in order to properly process them 
	according to the rule defined by the "Query parameters handling" option.

	<h2>Ver também</h2>
	<table>
		<tr>
			<td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td><a href="../../overview.html">Visão geral da interface do usuário</a></td>
			<td>para uma visão geral da interface do usuário</td>
		</tr>
		<tr>
			<td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td><a href="../../../start/concepts/spider.html">Spider</a></td>
			<td>para uma visão geral do spider</td>
		</tr>
		<tr>
			<td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td><a href="../../../ui/tabs/spider.html">Aba Spider</a></td>
			<td>para uma visão geral do guia do spider</td>
		</tr>
	</table>

</body>
</html>
